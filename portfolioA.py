import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score


# Configuration de la page
st.set_page_config(page_title="Mon Portfolio", page_icon=":briefcase:", layout="centered")


# Inject custom CSS to hide the GitHub logo and footer
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;} /* Cache le menu principal (contenant le logo GitHub) */
    footer {visibility: hidden;} /* Cache le pied de page */
    .viewerBadge_container__1QSob {display: none;} /* Cache le logo GitHub Streamlit Cloud */
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)


# Fonction pour charger et encoder les donn√©es
@st.cache_data
def load_and_encode_data(file_path):
    # Lien vers le fichier CSV brut (raw) sur GitHub
    url = "https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/loan_data_final.csv"
    # Lecture du fichier CSV depuis l'URL
    data = pd.read_csv(url)
    
    # Renomm√© la variable 'pret_non_rembours√©' en 'statut_pr√™t'
    data.rename(columns={'pret_non_rembours√©': 'statut_pr√™t'}, inplace=True)
    
    # Modifier la valeur de la colonne 'statut_pr√™t' : changer 'oui' par 'non_rembours√©' et 'non' par 'rembours√©'
    data['statut_pr√™t'] = data['statut_pr√™t'].replace({'oui': 'non_rembours√©', 'non': 'rembours√©'})
    
    label_encoder = LabelEncoder()
    
    # Encodage des colonnes cat√©gorielles
    data['objet__du_pret'] = label_encoder.fit_transform(data['objet__du_pret'])
    data['politique_de_credit'] = label_encoder.fit_transform(data['politique_de_credit'])
    
    return data

# Fonction pour pr√©parer les donn√©es d'entra√Ænement et de test
@st.cache_data
def prepare_data(data):
    X = data.drop(columns=['statut_pr√™t'])
    y = data['statut_pr√™t'].map({'non_rembours√©': 1, 'rembours√©': 0})
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # Combiner X_train et y_train pour faciliter le sur-√©chantillonnage
    train_data = pd.concat([X_train, y_train], axis=1)
    
    majority_class = train_data[train_data['statut_pr√™t'] == 0]
    minority_class = train_data[train_data['statut_pr√™t'] == 1]
    
    if not minority_class.empty:
        minority_class_upsampled = resample(minority_class, replace=True, n_samples=len(majority_class), random_state=42)
        train_data_balanced = pd.concat([majority_class, minority_class_upsampled])
        
        X_train_balanced = train_data_balanced.drop('statut_pr√™t', axis=1)
        y_train_balanced = train_data_balanced['statut_pr√™t']
        
        return X_train_balanced, X_test, y_train_balanced, y_test
    else:
        st.error("Erreur : La classe minoritaire est absente dans l'ensemble d'entra√Ænement.")
        return None, None, None, None

# Fonction pour entra√Æner et √©valuer le mod√®le
@st.cache_resource
def train_model(X_train, y_train, X_test, y_test):
    model = RandomForestClassifier(class_weight='balanced', random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred, output_dict=True, target_names=['rembours√© (0)', 'non_rembours√© (1)'])
    roc_auc = roc_auc_score(y_test, y_pred)
    
    return conf_matrix, class_report, roc_auc


# Fonction pour afficher les images de logos
def display_logos():
    col1, col2, col3, col4, col5, col6 = st.columns(6)
    logo_paths = [
        "https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/Matplotlib.png",
        "https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/Numpy.png",
        "https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/Pandas.png",
        "https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/Plotly.png",
        "https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/seaborn.png",
        "https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/Sklearn.png"
    ]
    
    for col, logo in zip([col1, col2, col3, col4, col5, col6], logo_paths):
        col.image(logo, width=100)

# Fonction pour g√©rer l'affichage de la page d'accueil
def show_home_page():
    # Cr√©ation des colonnes pour organiser la mise en page
    col1, col2 = st.columns([1, 2])

    with col1:
        # Ajout de la photo √† gauche
        st.image("https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/PhotoModifi√©.png", width=250)
        # Affichage du pr√©nom, nom (sur une seule ligne), titre et contact sous la photo
        st.markdown("<h2 style='text-align: center; font-size: 24px;'>Christian Tchouaff√©</h2>", unsafe_allow_html=True)
        st.markdown("<h4 style='text-align: center;'>Data Analyst</h4>", unsafe_allow_html=True)
        st.markdown(
            """
            <p style="font-size: 16px; line-height: 1.5;">
            üìû Tel : 07 86 15 97 69<br>
            üìß Email : <a href="mailto:christiantchouaffe@orange.fr">christiantchouaffe@orange.fr</a><br>
            üåê R√©seaux : <a href="https://www.linkedin.com/in/christiantchouaffe" target="_blank">linkedin.com/in/christiantchouaffe</a><br>
            <a href="https://github.com/FromTchouaffe" target="_blank">github.com/FromTchouaffe</a>
            </p>
            """,
            unsafe_allow_html=True,
        )

    with col2:
        # Choix de la langue au-dessus du texte de pr√©sentation
        langue = st.radio("Choisissez votre langue / Choose your language", ("Fran√ßais", "English"))

        # D√©caler et justifier le texte de pr√©sentation avec une taille de police augment√©e et le remonter pour centrer par rapport √† la photo
        if langue == "Fran√ßais":
            st.markdown(
                """
                <div style="margin-left: 80px; margin-top: 20px; text-align: justify; font-size: 18px;">
                <p style="font-size: 22px; font-weight: bold; text-align: justify;">Exploiter la donn√©e pour en extraire de la valeur.</p>
                En tant que Data Analyst polyvalent, je ma√Ætrise une large gamme d'analyses : descriptives, hypoth√©tico-d√©ductives, 
                inf√©rentielles et exploratoires, avec une attention particuli√®re √† la d√©tection de biais dans les donn√©es. 
                Mes analyses s'appuient sur des tests statistiques rigoureux et se traduisent par la r√©daction de rapports d√©taill√©s, 
                la cr√©ation de tableaux de bord interactifs via des outils BI comme Power BI, ou par l'automatisation de t√¢ches √† l'aide 
                de frameworks tels que Streamlit ou Voil√†.
                Je r√©alise √©galement des requ√™tes SQL sur des ERP clients et j'utilise des algorithmes de machine learning pour effectuer 
                des pr√©dictions gr√¢ce aux techniques d'apprentissage supervis√© et non supervis√©. Actuellement, je m'attache √† int√©grer 
                la GenAI en d√©veloppant une plateforme innovante, con√ßue pour proposer des assistants IA personnalis√©s aux besoins sp√©cifiques 
                des utilisateurs.
                </div>
            """, 
            unsafe_allow_html=True
        )
        else:
            st.markdown(
                """
                <div style="margin-left: 80px; margin-top: 20px; text-align: justify; font-size: 18px;">
                <p style="font-size: 22px; font-weight: bold; text-align: justify;">Leveraging data to extract value.</p>
                As a versatile Data Analyst, I am proficient in a wide range of analyses: descriptive, hypothetico-deductive, 
                inferential, and exploratory, with a particular focus on detecting biases in data. My analyses are grounded in rigorous 
                statistical testing and result in the creation of detailed reports, interactive dashboards using BI tools such as Power BI, 
                or task automation through frameworks like Streamlit or Voil√†.
                I also perform SQL queries on client ERPs and use machine learning algorithms to make predictions using both supervised 
                and unsupervised learning techniques. Currently, I am focused on integrating GenAI by developing an innovative platform 
                designed to offer AI assistants tailored to the specific needs of users.
                </div>
            """, 
            unsafe_allow_html=True
        )

    
    # Afficher les logos des biblioth√®ques
        st.markdown("---")
        st.markdown(
        "<h5 style='font-size: 16px; text-align: justify; margin-bottom: 5px;'>Je programme en Python et pour la r√©alisation des cas d'usage de ce portfolio j'ai utlis√© des librairies telles que :</h5>",
        unsafe_allow_html=True
        )
        display_logos()

def show_supervised_learning_page(data, X_train_balanced, y_train_balanced, X_test, y_test):
    st.title("Apprentissage supervis√©")
    st.write("**Objectif :** L'apprentissage supervis√© consiste √† entra√Æner un algorithme d'IA √† estimer des valeurs futures √† partir des donn√©es pass√©es, qu'il s'agisse de biens, de services, ou de comportements.")
    # Ajout du paragraphe Cas d'usage
    st.write("**Cas d'usage :** Pr√©diction de la capacit√© d'un emprunteur √† rembourser son pr√™t en se basant sur les donn√©es disponibles. (source du jeu de donn√©es : Kaggle.com)")

    # Menu d√©roulant pour la s√©lection de la section
    section = st.selectbox("S√©lectionnez une section", ["Pr√©sentation du jeu de donn√©es", "Visualisation", "Pr√©diction"])
    
    if section == "Pr√©sentation du jeu de donn√©es":
        st.header("Pr√©sentation du jeu de donn√©es")
        st.write("Voici les premi√®res lignes du dataset :")
        st.dataframe(data.head())
        
        info = pd.DataFrame({
            "Nom de la variable": data.columns,
            "Nombre d'occurrences": data.count(),
            "Type de variable": data.dtypes
        }).reset_index(drop=True)
        
        info = info.astype(str)
        
        # Centrer le tableau en utilisant Streamlit
        st.markdown("<div style='text-align: center;'>Informations sur les variables du dataset :</div>", unsafe_allow_html=True)
        st.dataframe(info)

        # Ajouter le texte en dessous dans un paragraphe simple et justifi√©
        st.markdown(
            """
            <p style="text-align: justify;">
            Le jeu de donn√©es contient des informations d√©taill√©es sur des pr√™ts accord√©s par une banque, avec un focus sur les caract√©ristiques financi√®res 
            et comportementales des emprunteurs. Il inclut 14 variables, dont 13 sont explicatives et une variable cible. 
            Les variables explicatives couvrent divers aspects tels que les objectifs des pr√™ts, les taux d'int√©r√™t, les mensualit√©s, 
            le revenu annuel (logarithm√©), le ratio d'endettement, le score de cr√©dit, et l'historique de cr√©dit de l'emprunteur.
            
            La variable cible, <strong>statut_pr√™t</strong>, indique si le pr√™t a √©t√© rembours√© ou non, ce qui permet d'analyser les facteurs 
            contribuant au risque de non-remboursement.

            Le jeu de donn√©es pr√©sente un d√©s√©quilibre de classes, avec moins d'exemples de pr√™ts non rembours√©s. 
            Il est cependant plus important de pr√©dire avec pr√©cision si un pr√™t ne sera pas rembours√© que l'inverse.
            </p>
            """, 
            unsafe_allow_html=True
        )

    elif section == "Visualisation":
        st.header("Visualisation")
        
        numeric_columns = data.select_dtypes(include=['float64', 'int64'])
        correlation_matrix = numeric_columns.corr()
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
        plt.title('Matrice de corr√©lation des variables num√©riques')
        st.pyplot(plt.gcf())
        
        # Ajouter le commentaire apr√®s les graphiques
        st.markdown(
            """
            La corr√©lation la plus importante est fortement n√©gative entre le taux d'int√©r√™t et le score de cr√©dit. 
            Cela signifie que les individus ayant un meilleur score de cr√©dit tendent √† obtenir des taux d'int√©r√™t plus bas, ce qui est logique dans le cadre d'une √©valuation du risque par les pr√™teurs. 
            Il n'y a pas d'autres corr√©lations significatives entre les diff√©rentes variables (au-dessus de 0,7 ou en dessous de -0,7), 
            ce qui indique que les variables num√©riques du jeu de donn√©es sont en grande partie ind√©pendantes les unes des autres.

            Le score de cr√©dit pourrait √™tre une variable particuli√®rement importante pour pr√©dire si un pr√™t sera rembours√©, car il est fortement li√© √† plusieurs autres variables cl√©s (taux d'int√©r√™t, utilisation du cr√©dit).
            
            """
        )
        st.header("Histogrammes des variables num√©riques")
        plt.figure(figsize=(15, 20))
        for i, column in enumerate(numeric_columns, 1):
            plt.subplot(5, 3, i)
            sns.histplot(data, x=column, hue='statut_pr√™t', kde=False, multiple="stack", palette="coolwarm")
            plt.title(f'Histogramme de {column}')
            plt.xlabel(column)
            plt.ylabel('Fr√©quence')
        
        plt.tight_layout()
        st.pyplot(plt.gcf())
        plt.close() # Fermer le graphique pour √©viter les superpositions
        
        # Ajouter le commentaire apr√®s les graphiques
        st.markdown(
            """
            Les histogrammes des variables num√©riques r√©v√®lent un d√©s√©quilibre entre les pr√™ts rembours√©s et non rembours√©s.
            Cependant, les distributions au sein de chaque variable restent globalement similaires.

            
            """
        )

    elif section == "Pr√©diction":
        st.header("Pr√©diction")

        # Pr√©sentation des algorithmes apr√®s les r√©sultats de pr√©diction
        st.subheader("Pr√©sentation des Algorithmes")
        st.markdown("""
            <div style="text-align: justify;">
            <ul>
            <li><strong>Random Forest</strong> : algorithme d'apprentissage supervis√© qui fonctionne en construisant plusieurs arbres de d√©cision sur diff√©rents √©chantillons de donn√©es. 
            Chacun de ces arbres fait des pr√©dictions, et le Random Forest combine les pr√©dictions des arbres pour donner une r√©ponse finale.</li>

            <li><strong>Decision Tree</strong> : un mod√®le simple qui divise les donn√©es en groupes bas√©s sur des conditions. Chaque "n≈ìud" dans l'arbre repr√©sente une d√©cision sur une variable, et chaque "branche" m√®ne √† une pr√©diction.</li>

            <li><strong>Gradient Boosting</strong> : algorithme plus avanc√© qui cr√©e une s√©rie d'arbres de d√©cision, o√π chaque arbre corrige les erreurs du pr√©c√©dent. √Ä chaque √©tape, l'algorithme se "booste" pour am√©liorer la pr√©cision.</li>

            <li><strong>R√©gression Logistique</strong> : mod√®le statistique simple, utilis√© pour pr√©dire des r√©sultats binaires (deux classes, comme "oui/non" ou "rembours√©/non rembours√©"). Contrairement √† la r√©gression lin√©aire, qui pr√©dit des valeurs continues, la r√©gression logistique pr√©dit des probabilit√©s.</li>
            </ul>
            </div>
        """, unsafe_allow_html=True)


        
    # Menu d√©roulant pour choisir le mod√®le √† tester
        model_choice = st.selectbox(
            "Choisissez un mod√®le √† tester",
            ["Random Forest", "Decision Tree", "Gradient Boosting", "R√©gression Logistique"]
        )

    # Entra√Æner et afficher les r√©sultats selon le mod√®le choisi
        if X_train_balanced is not None and y_train_balanced is not None:
            if model_choice == "Random Forest":
            # Entra√Æner le Random Forest avec les donn√©es √©quilibr√©es
                model = RandomForestClassifier(class_weight='balanced', random_state=42)
                model.fit(X_train_balanced, y_train_balanced)
                y_pred = model.predict(X_test)
            elif model_choice == "Decision Tree":
            # Entra√Æner le Decision Tree avec les donn√©es √©quilibr√©es
                dt_classifier = DecisionTreeClassifier(random_state=42)
                dt_classifier.fit(X_train_balanced, y_train_balanced)
                y_pred = dt_classifier.predict(X_test)
            elif model_choice == "Gradient Boosting":
            # Entra√Æner le Gradient Boosting avec les donn√©es √©quilibr√©es
                gb_classifier = GradientBoostingClassifier(random_state=42)
                gb_classifier.fit(X_train_balanced, y_train_balanced)
                y_pred = gb_classifier.predict(X_test)
            elif model_choice == "R√©gression Logistique":
            # Entra√Æner la R√©gression Logistique avec les donn√©es √©quilibr√©es
                logreg_classifier = LogisticRegression(random_state=42, max_iter=1000)
                logreg_classifier.fit(X_train_balanced, y_train_balanced)
                y_pred = logreg_classifier.predict(X_test)

        # G√©n√©rer la matrice de confusion et le rapport de classification
            conf_matrix = confusion_matrix(y_test, y_pred)
            class_report = classification_report(y_test, y_pred, output_dict=True, target_names=['Rembours√©', 'Non rembours√©'])
            roc_auc = roc_auc_score(y_test, y_pred)

        # Afficher les r√©sultats sous forme de tableau
            st.subheader(f"R√©sultats pour {model_choice}")
            st.write("Matrice de confusion :")
            conf_matrix_df = pd.DataFrame(conf_matrix, index=['Rembours√©', 'Non rembours√©'], columns=['Pr√©dit Rembours√©', 'Pr√©dit Non rembours√©'])
            st.dataframe(conf_matrix_df)

            st.write("Rapport de classification :")
            class_report_df = pd.DataFrame(class_report).transpose().apply(lambda x: np.round(x, 2))
            st.dataframe(class_report_df)

            st.write(f"Score AUC ROC : {roc_auc:.2f}")

        # Ajouter un commentaire sp√©cifique pour chaque mod√®le
            if model_choice == "Random Forest":
                st.markdown("""
                <div style="text-align: justify;">
                <ul>
                <li><strong>Explications des m√©triques</strong></li>
            
                <li><strong>Pr√©cision</strong> : La pr√©cision mesure la proportion de pr√©dictions correctes parmi toutes les pr√©dictions faites pour une classe donn√©e. 
                Par exemple, une pr√©cision de 85% pour les pr√™ts rembours√©s signifie que, parmi toutes les pr√©dictions de pr√™ts rembours√©s faites par le mod√®le, 85% √©taient correctes.</li>

                <li><strong>Rappel</strong> : Le rappel, ou sensibilit√©, mesure la proportion de v√©ritables cas positifs qui sont correctement identifi√©s par le mod√®le. 
                Un rappel de 98% pour les pr√™ts rembours√©s signifie que le mod√®le a correctement identifi√© 98% des pr√™ts qui ont effectivement √©t√© rembours√©s.</li>

                <li><strong>F1-score</strong> : Le F1-score est la moyenne harmonique de la pr√©cision et du rappel, offrant un √©quilibre entre ces deux m√©triques. 
                Il est particuli√®rement utile lorsque les classes sont d√©s√©quilibr√©es, car il p√©nalise √† la fois les faux positifs et les faux n√©gatifs. 
                Un F1-score de 91% pour les pr√™ts rembours√©s indique une forte performance combin√©e en termes de pr√©cision et de rappel.</li>

                <li><strong>AUC-ROC</strong> : L'AUC-ROC (Area Under the Curve - Receiver Operating Characteristic) est une m√©trique qui √©value la capacit√© du mod√®le √† distinguer entre les classes. 
                Un score AUC de 0.5 indique une performance al√©atoire, tandis qu'un score de 1.0 indique une distinction parfaite. 
                Dans notre cas, un AUC de 0.52 pour le mod√®le signifie qu'il a du mal √† diff√©rencier efficacement entre les pr√™ts rembours√©s et non rembours√©s.</li>
                </ul>
                </div>
            """, unsafe_allow_html=True)

            elif model_choice == "Decision Tree":
                st.markdown("Le Decision Tree offre des performances similaires, mais tend √† surapprendre sur les donn√©es d'entra√Ænement.")
            elif model_choice == "Gradient Boosting":
                st.markdown("Le Gradient Boosting am√©liore l√©g√®rement la pr√©diction des pr√™ts non rembours√©s gr√¢ce √† sa capacit√© d'ensemble.")
            elif model_choice == "R√©gression Logistique":
                st.markdown("La r√©gression logistique est un mod√®le simple mais robuste, avec une bonne capacit√© √† pr√©dire les pr√™ts rembours√©s, bien que la pr√©diction des non rembours√©s soit limit√©e.")
         
        


# Charger les donn√©es
data = load_and_encode_data('https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/loan_data_final.csv')

# Pr√©parer les donn√©es
X_train_balanced, X_test, y_train_balanced, y_test = prepare_data(data)

# Menu de navigation dans la barre lat√©rale
page = st.sidebar.radio("Navigation", ["Accueil", "Apprentissage supervis√©", "Apprentissage non supervis√©", "Apprentissage profond"])

if page == "Accueil":
    show_home_page()
elif page == "Apprentissage supervis√©":
    show_supervised_learning_page(data, X_train_balanced, y_train_balanced, X_test, y_test)


# Condition pour la page "Apprentissage non supervis√©"
elif page == "Apprentissage non supervis√©":
    st.title("Apprentissage non supervis√©")
    # Ajout du paragraphe Objectif
    st.write("**Objectif :** L'apprentissage non supervis√© consiste √† entra√Æner un algorithme d'IA √† partir de donn√©es non √©tiquet√©es, afin de d√©couvrir des structures cach√©es ou des regroupements dans les donn√©es, sans conna√Ætre √† l'avance notre cible.")
    # Ajout du paragraphe Cas d'usage
    st.write("**Cas d'usage :** Identification des patterns dans l'√©cosyst√®me de la recherche en Suisse √† travers la cr√©ation de clusters bas√©s sur un jeu de donn√©es. (source du jeu donn√©es : zenodo.org)")  
    # Chargement du jeu de donn√©es sp√©cifique √† l'apprentissage non supervis√©
    research_final = pd.read_csv('https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/research_final.csv', sep=';')

    # Menu d√©roulant pour s√©lectionner la sous-partie
    section_unsupervised = st.selectbox("S√©lectionnez une section", 
                                        ["Pr√©sentation du jeu de donn√©es", "Visualisation", "Mod√©lisation"])
    
    if section_unsupervised == "Pr√©sentation du jeu de donn√©es":
        st.header("Pr√©sentation du jeu de donn√©es")

        # Remplacer 'f' par 'female' et 'm' par 'male' dans la colonne 'gender'
        research_final['gender'] = research_final['gender'].replace({'f': 'femme', 'm': 'homme'})

        # Affichage des premi√®res lignes du DataFrame trait√©
        st.write("Voici les premi√®res lignes du jeu de donn√©es apr√®s traitement :")
        st.dataframe(research_final.head())
        
        # Affichage des informations sur les variables du dataset
        info = pd.DataFrame({
            "Nom de la variable": research_final.columns,
            "Nombre d'occurrences": research_final.count(),
            "Type de variable": research_final.dtypes
        }).reset_index(drop=True)
        
        info = info.astype(str)
        
        # Centrer le tableau en utilisant Streamlit
        st.markdown("<div style='text-align: center;'>Informations sur les variables du dataset :</div>", unsafe_allow_html=True)
        st.dataframe(info)

        # Ajout du commentaire en dessous du tableau
        st.markdown("### Commentaire")
        st.markdown("""
        <div style="text-align: justify;">
        Le DataFrame, r√©duit √† 14 variables essentielles parmi une trentaine initiale, offre une vue d'ensemble des chercheurs en termes de r√©sultat acad√©mique et de production scientifique. 
        Les variables incluent des informations d√©mographiques telles que l'identifiant unique, le genre et le groupe d'√¢ge, ainsi que des d√©tails sur le domaine de recherche principal 
        et le type d'institution d'affiliation. D'autres variables mesurent l'implication dans des projets financ√©s par l'ERC, les publications annuelles, les pr√©publications, 
        les citations re√ßues, ainsi que la note moyenne glissante, qui √©value la performance sur une p√©riode donn√©e.
        </div>
        """, unsafe_allow_html=True)

    elif section_unsupervised == "Visualisation":
        st.header("Visualisation")
        # S√©lection des colonnes 'max' pour la matrice de corr√©lation
        max_columns = ['max_rolling_7', 'max_articles_7', 'max_preprints_7', 'max_citations_7']
        correlation_matrix = research_final[max_columns].corr()  # Assurez-vous que 'research_final' est le bon DataFrame
    
        # Affichage de la matrice de corr√©lation
        plt.figure(figsize=(8, 6))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
        plt.title('Matrice de Corr√©lation des variables articles,citations,prepublications et notes acad√©miques')
        
        # Affichage du graphique dans Streamlit
        st.pyplot(plt.gcf())
        plt.clf()  # Nettoyage du graphique apr√®s affichage

        # Ajout du commentaire en dessous de la matrice de corr√©lation
        st.markdown("### Commentaire matrice de corr√©lation")
        st.markdown("""
        <div style="text-align: justify;">
        La relation la plus notable est celle entre le nombre maximum d'articles publi√©s et le nombre maximum de citations, 
        qui montre une corr√©lation mod√©r√©e positive. Pour le reste, il y a peu de corr√©lation entre les variables.
        </div>
        """, unsafe_allow_html=True)

        # Distribution des tranches d'√¢ge par genre
        research_final['gender'] = research_final['gender'].replace({'f': 'femme', 'm': 'homme'})
        age_order = ["< 45", "45-54", "55-64", "65+"]
        gender_order = ["homme", "femme"]

        plt.figure(figsize=(10, 6))
        sns.countplot(data=research_final, x='time_dep_age_group', hue='gender', order=age_order, hue_order=gender_order)
        plt.title('Distribution des Tranches d\'√Çge par Genre')
        plt.xlabel('Tranche d\'√Çge')
        plt.ylabel('Nombre')

        # Affichage du graphique dans Streamlit
        st.pyplot(plt.gcf())
        plt.clf()

        # Ajout du commentaire en dessous du graphique de distribution des √¢ges
        st.markdown("### Commentaire distribution des √¢ges")
        st.markdown("""
        <div style="text-align: justify;">
        Le jeu de donn√©es recense 4460 chercheurs dont 3397 hommes (76%) et 1063 femmes (24%).
        </div>
        """, unsafe_allow_html=True)

        # Graphique de boxplot pour les distributions par genre avec une autre palette
        custom_palette = {"femme": "lightblue", "homme": "lightcoral"}

        # Cr√©er les subplots
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        sns.boxplot(x='gender', y='max_rolling_7', data=research_final, ax=axes[0], hue='gender', palette=custom_palette, dodge=False)
        axes[0].set_title("Distribution de la note glissante maximale par genre")
        axes[0].set_xlabel("Genre")
        axes[0].set_ylabel("Note glissante maximale sur 7 ans")

        sns.boxplot(x='gender', y='max_articles_7', data=research_final, ax=axes[1], hue='gender', palette=custom_palette, dodge=False)
        axes[1].set_title("Distribution du nombre maximum d'articles par genre")
        axes[1].set_xlabel("Genre")
        axes[1].set_ylabel("Nombre maximum d'articles sur 7 ans")

        plt.tight_layout()
        st.pyplot(fig)
        plt.clf()

        st.markdown("### Commentaire distribution notes acad√©miques et articles")
        st.markdown("""
        <div style="text-align: justify;">
        <ul>
        <li> La moyenne de la note glissante maximale est l√©g√®rement sup√©rieure chez les hommes (3.86) compar√©e aux femmes (3.65). </li>
        <li> Il existe une diff√©rence significative dans le nombre maximum d'articles publi√©s sur 7 ans, avec une moyenne beaucoup plus √©lev√©e chez les hommes (5.21) que chez les femmes (2.72). </li>
        </ul>
        </div>
        """, unsafe_allow_html=True)
        
        # Visualisation du nombre d'articles et de citations par type d'institut et par ann√©e
        st.subheader("Nombre d'articles et de citations par type d'institut et par ann√©e")
    
        institute_types = research_final['institute_type'].unique()

        fig, ax = plt.subplots(len(institute_types), 2, figsize=(14, len(institute_types) * 5), sharex=True)
    
        for i, institute in enumerate(institute_types):
            institute_data = research_final[research_final['institute_type'] == institute].groupby('year')[['max_articles_7', 'max_citations_7']].sum()

            institute_data['max_articles_7'].plot(ax=ax[i, 0], marker='o', title=f"{institute}: Nombre d'articles publi√©s par an")
            ax[i, 0].set_ylabel("Nombre d'articles")
            ax[i, 0].grid(True)

            institute_data['max_citations_7'].plot(ax=ax[i, 1], marker='o', title=f"{institute}: Nombre de citations par an")
            ax[i, 1].set_ylabel("Nombre de citations")
            ax[i, 1].grid(True)

            ax[i, 0].set_xticks(institute_data.index)
            ax[i, 1].set_xticks(institute_data.index)
            ax[i, 0].set_xticklabels(institute_data.index.astype(int))
            ax[i, 1].set_xticklabels(institute_data.index.astype(int))
    
        ax[-1, 0].set_xlabel("Ann√©e")
        ax[-1, 1].set_xlabel("Ann√©e")

        plt.tight_layout()
        st.pyplot(fig)
        plt.clf()

        st.markdown("### Commentaire distribution articles vs citations")
        st.markdown("""
        <div style="text-align: justify;">
        <ul>
        <li>Pour les universit√©s cantonales, on observe une croissance constante du nombre d'articles publi√©s et des citations re√ßues.</li>
        <li>Le domaine ETH publie moins d'articles, mais re√ßoit beaucoup de citations, montrant leur influence.</li>
        <li>Les autres instituts ont une productivit√© scientifique plus modeste.</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

    elif section_unsupervised == "Mod√©lisation":
        st.header("Mod√©lisation")

        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import matplotlib
        import seaborn as sns
        from sklearn.preprocessing import StandardScaler
        from sklearn.decomposition import PCA

        if 'research_final' in locals():
            research_final['gender'] = research_final['gender'].replace({'f': 'femme', 'm': 'homme'})
            pca_data = research_final[['anonym_id', 'year', 'gender', 'time_dep_age_group', 'main_research_area', 'institute_type', 'running_erc_project', 'max_rolling_7', 'max_articles_7', 'max_preprints_7', 'max_citations_7']].copy()

            st.write("Les donn√©es ont √©t√© pr√©par√©es pour l'analyse de clustering. Voici un aper√ßu des premi√®res lignes :")
            st.dataframe(pca_data.head())
        else:
            st.error("La variable 'research_final' n'a pas √©t√© d√©finie.")

        st.markdown("""
        <p style="text-align: justify;">
        Les donn√©es ont √©t√© filtr√©es pour inclure uniquement les colonnes pertinentes au clustering. Ces donn√©es serviront de base √† l'analyse des similarit√©s entre les chercheurs.
        </p>
        """, unsafe_allow_html=True)

        numeric_data = pca_data[['max_rolling_7', 'max_articles_7', 'max_preprints_7', 'max_citations_7']]
        scaler = StandardScaler()
        standardized_data = scaler.fit_transform(numeric_data)
        standardized_data = pd.DataFrame(standardized_data, columns=numeric_data.columns)

        st.write("Donn√©es standardis√©es pour la mod√©lisation :")
        st.dataframe(standardized_data.head())

        pca = PCA(n_components=2)
        pca_result = pca.fit_transform(standardized_data)

        variables = standardized_data.columns
        st.subheader("Cercle des corr√©lations")

        plt.figure(figsize=(8, 8))
        colors = matplotlib.cm.get_cmap('tab10')

        for i, var in enumerate(variables):
            plt.quiver(0, 0, pca.components_[0, i], pca.components_[1, i], angles='xy', scale_units='xy', scale=1, color=colors(i % len(variables)), label=var)

        circle = plt.Circle((0, 0), 1, color='b', fill=False)
        plt.gca().add_artist(circle)

        plt.xlim(-1.1, 1.1)
        plt.ylim(-1.1, 1.1)
        plt.xlabel('Premi√®re composante principale (PC1)')
        plt.ylabel('Deuxi√®me composante principale (PC2)')
        plt.title('Cercle des corr√©lations')

        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)

        plt.grid(True)
        plt.axhline(0, color='black', linewidth=0.5)
        plt.axvline(0, color='black', linewidth=0.5)

        st.pyplot(plt.gcf())
        plt.clf()

        st.markdown("### Commentaire sur le cercle des corr√©lations")
        st.markdown("""
        <div style="text-align: justify;">
        <ul>
        <li> La premi√®re composante principale (PC1) explique 41.72% de la variance totale.</li>
        <li> Les variables li√©es aux citations et aux articles sont dominantes dans l'analyse.</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

        st.subheader("Clustering avec K-means")

        from sklearn.cluster import KMeans

        optimal_clusters = 4
        kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
        clusters = kmeans.fit_predict(standardized_data)

        pca_data['Cluster'] = clusters
        cluster_counts = pca_data['Cluster'].value_counts()
        st.write("R√©partition des individus par cluster :")
        st.bar_chart(cluster_counts)

        st.subheader("Visualisation des clusters avec PCA")

        plt.figure(figsize=(10, 8))
        colors = ['red', 'blue', 'green', 'purple']

        for cluster in range(optimal_clusters):
            cluster_data = pca_result[pca_data['Cluster'] == cluster]
            plt.scatter(cluster_data[:, 0], cluster_data[:, 1], color=colors[cluster], label=f'Cluster {cluster}', alpha=0.6, s=100)

        plt.title('Visualisation des clusters apr√®s PCA')
        plt.xlabel('Premi√®re composante principale (PC1)')
        plt.ylabel('Deuxi√®me composante principale (PC2)')
        plt.legend()
        st.pyplot(plt.gcf())
        plt.clf()

        st.markdown("### Commentaire sur les clusters")
        st.markdown("""
        <div style="text-align: justify;">
        <li>  Cluster 0 : Productivit√© mod√©r√©e mais constante.</li>
        <li> Cluster 1 : Faible productivit√©.</li>
        <li> Cluster 2 : Similaire au Cluster 0 mais avec moins d'articles publi√©s.</li>
        <li> Cluster 3 : Le plus productif et influent.</li>
        </div>
        """, unsafe_allow_html=True)

        st.subheader("Tests de chi¬≤ pour les variables cat√©gorielles vs Cluster")

        import pandas as pd
        from scipy.stats import chi2_contingency

        contingency_table_research_area = pd.crosstab(pca_data['main_research_area'], pca_data['Cluster'])
        chi2_research_area, p_research_area, dof_research_area, _ = chi2_contingency(contingency_table_research_area)

        contingency_table_institute_type = pd.crosstab(pca_data['institute_type'], pca_data['Cluster'])
        chi2_institute_type, p_institute_type, dof_institute_type, _ = chi2_contingency(contingency_table_institute_type)

        contingency_table_age_group = pd.crosstab(pca_data['time_dep_age_group'], pca_data['Cluster'])
        chi2_age_group, p_age_group, dof_age_group, _ = chi2_contingency(contingency_table_age_group)

        st.write("P-values des tests de chi¬≤ pour les variables cat√©gorielles :")
        chi2_results = {
            'P-value main_research_area vs Cluster': p_research_area,
            'P-value institute_type vs Cluster': p_institute_type,
            'P-value time_dep_age_group vs Cluster': p_age_group
        }
        st.write(chi2_results)

        st.markdown("### Conclusion sur le cluster")
        st.markdown("""
        <div style="text-align: justify;">
        Les r√©sultats des tests du chi-carr√© montrent qu‚Äôil y a une forte association entre les variables cat√©gorielles (domaine de recherche, type d'institut, et √¢ge) et les clusters.
        </div>
        """, unsafe_allow_html=True)



# Bloc "Apprentissage profond"
elif page == "Apprentissage profond":
    st.title("Big Data et Intelligence Artificielle")

    # Menu d√©roulant pour les sections d'apprentissage profond
    section_deep_learning = st.selectbox(
    "S√©lectionnez une section", 
    ["Contexte et enjeux", "Les Large Language Models", "Cas d'usage"]
    )

    # Section "Contexte et enjeux"
    if section_deep_learning == "Contexte et enjeux":
        st.header("Contexte et Enjeux")
        st.markdown("""
            <div style="text-align: justify;">
            Dans ce portfolio, j'ai explor√© deux axes fondamentaux du machine learning : l'apprentissage supervis√© 
            et l'apprentissage non supervis√©. L'apprentissage par renforcement n'a pas √©t√© abord√©, en raison des ressources 
            et des volumes de donn√©es significativement plus √©lev√©s qu'il requiert pour une impl√©mentation robuste.

            Deux mod√®les ont √©t√© d√©velopp√©s, chacun bas√© sur des algorithmes adapt√©s √† des probl√©matiques sp√©cifiques :
        
            - **RandomForestClassifier** : utilis√© pour pr√©dire la probabilit√© de remboursement d'un pr√™t.
            - **KMeans** : destin√© √† segmenter les chercheurs en cat√©gories pertinentes dans le cadre d'une √©tude de positionnement au sein de l'√©cosyst√®me de la recherche en Suisse.
            </div>
        """, unsafe_allow_html=True)

        # Insertion de l'image √† la fin des deux tirets
        st.image("https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/AlgoML.png", caption="Illustration des principaux algorithmes de machine learning", use_column_width=True)
        # Poursuite du texte apr√®s l'image
        st.markdown("""
            <div style="text-align: justify;">
            Ce portfolio illustre le processus d'entra√Ænement des mod√®les, qui, une fois affin√©s, seront int√©gr√©s dans des 
            applications d'aide √† la d√©cision. Le mod√®le de classification permettra de d√©terminer l'√©ligibilit√© des clients 
            √† un pr√™t, tandis que l'algorithme de clustering contribuera √† optimiser l'int√©gration des chercheurs en fonction 
            de leur profil et de leur domaine.
            
            L‚Äôactualit√© r√©cente, avec l‚Äôarriv√©e de ChatGPT, a mis en lumi√®re le deep learning, reposant sur des architectures de r√©seaux de neurones complexes. Contrairement aux algorithmes classiques comme les Random Forests ou les Decision Trees, souvent interpr√©tables ("white box"), les mod√®les de deep learning (CNN, RNN, Transformers) sont des "black box" en raison de leur complexit√© et de leur opacit√©, ce qui les distingue de l‚Äôapprentissage supervis√© explor√© dans ce portfolio.
            </div>
        """, unsafe_allow_html=True)

    # Section "Les Large Language Models"
    elif section_deep_learning == "Les Large Language Models":
        st.header("Les Large Language Models")
        st.markdown("""
            <div style="text-align: justify;">
            Les algorithmes de deep learning ont connu un essor remarquable avec l'av√®nement des Large Language Models (LLMs), 
            qui ont r√©volutionn√© les techniques de traitement du langage naturel (NLP). Ces mod√®les ont non seulement √©tabli 
            de nouveaux standards dans diverses t√¢ches li√©es au langage, mais ils ont √©galement surpass√© leurs pr√©d√©cesseurs 
            en ouvrant la voie √† des avanc√©es sans pr√©c√©dent dans l'IA.
            </div>
        """, unsafe_allow_html=True)

        # Insertion de l'image avec une l√©gende
        st.image("https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/IALandscape_2.png", 
            caption="Les LLMs dans le paysage de l'IA", 
            use_column_width=True)

        st.markdown("""
            <div style="text-align: justify;">
            Le qualificatif de "large" ou "grand" pour les LLMs fait r√©f√©rence √† l'√©norme quantit√© de donn√©es d'entra√Ænement 
            et de ressources n√©cessaires √† leur fonctionnement. Le terme "mod√®les" souligne leur capacit√© √† apprendre des 
            motifs complexes √† partir des donn√©es. Dans le cas des LLMs, ces donn√©es proviennent principalement de vastes 
            corpus textuels issus d'internet.

            Ces mod√®les sont √©galement qualifi√©s de multimodaux car ils sont capables de traiter et de g√©n√©rer des informations 
            √† partir de divers types de donn√©es, tels que le texte, l'audio, la vid√©o, et les images. Cela les diff√©rencie des 
            mod√®les non multimodaux, qui ne fonctionnent qu'avec un seul type de donn√©es, g√©n√©ralement du texte.
            </div>
        """, unsafe_allow_html=True)

        # Cr√©ation des donn√©es sous forme de dictionnaire
        data = {
            "Marque": ["Microsoft", "Google", "Google", "Meta (Facebook)", "Meta (Facebook)"],
            "Mod√®le de Fondation": ["GPT-3", "BERT", "T5", "RoBERTa / BlenderBot", "LLaMA"],
            "Mod√®le Fine-tun√©": [
                "Codex pour la g√©n√©ration de code", 
                "InstructBERT (pour la compr√©hension des instructions)", 
                "T5 fine-tun√© pour des t√¢ches sp√©cifiques comme la traduction ou le r√©sum√© de texte", 
                "BlenderBot (tun√© pour des conversations humaines)", 
                "LLaMA fine-tun√© pour des t√¢ches de g√©n√©ration de texte sp√©cifiques"
            ],
            "Motif": [
                "G√©n√©ration de code", 
                "Compr√©hension des instructions", 
                "Traduction ou r√©sum√© de texte", 
                "Conversations humaines", 
                "G√©n√©ration de texte"
            ]
        }

        # Cr√©ation du DataFrame √† partir des donn√©es
        df = pd.DataFrame(data)

        # CSS pour centrer et justifier le tableau
        table_style = """
        <style>
        table {
            margin-left: auto;
            margin-right: auto;
            width: 100%;
            text-align: center;
        }
        thead th {
            text-align: center;
        }
        tbody td {
            text-align: justify;
        }
        </style>
        """
        # Affichage du tableau dans Streamlit sans index
        st.markdown("### Mod√®les de fondation")
        st.markdown(df.to_html(index=False), unsafe_allow_html=True)

        # Texte √† afficher entre les deux tableaux avec des tirets sous forme d'ast√©risques
        texte = """
        La cr√©ation d‚Äôun mod√®le fondationel (pr√©-entrain√©) n√©cessite des ordinateurs puissants et une infrastructure sp√©cialis√©e :
        * Des serveurs puissants : processeurs rapides (CPU), nombreuses unit√©s de traitement graphique (GPU et TPU) 
        * Un stockage massif : syst√®mes de stockage capables de g√©rer des t√©raoctets (1 To √©quivaut √† 1 million de livres de 200 √† 500 pages) voire des p√©taoctets de donn√©es.
        * Des r√©seaux de haute performance : connexion rapide entre serveurs pour √©viter la lenteur lors de la phase d‚Äôentra√Ænement 
        * Des logiciels et frameworks adapt√©s : des environnements cloud (Google Cloud, AWS, Microsoft Azure) et des frameworks (PyTorch, TensorFlow) pour traiter des quantit√©s massives de donn√©es n√©cessitant des ressources computationnelles importantes.
        * La disponibilit√© des donn√©es d'entra√Ænement de haute qualit√© : garbage in, garbage out
        """

        # Affichage du texte
        st.markdown(texte)

        # Cr√©ation des donn√©es sous forme de dictionnaire
        data = {
            "Crit√®re": ["Puissance", "Temps de pr√©paration", "Quantit√© de donn√©es"],
            "Mod√®le de fondation": [
                "Des milliers de CPU et GPU", 
                "Des semaines ou des mois", 
                "Des centaines de Go"
             ],
            "Mod√®le fine-tun√© ou sp√©cialis√©": [
                "1 CPU ou GPU", 
                "Quelques jours", 
                "Quelques centaines de Mo ou quelques Go"
            ]
        }

        # Cr√©ation du DataFrame
        df = pd.DataFrame(data)

        # Affichage du tableau dans Streamlit
        st.markdown("### Comparaison entre Mod√®le de fondation et Mod√®le fine-tun√© ou sp√©cialis√©")
        st.markdown(df.to_html(index=False), unsafe_allow_html=True)

        st.image("https://raw.githubusercontent.com/FromTchouaffe/portfolio_new/main/LLMsTrans.png", 
            caption="Sch√©ma de d√©ploiement d'une IA bas√©e sur un LLM", 
            use_column_width=True)

    
        st.markdown("""
            <div style="text-align: justify;">
            Les Large Language Models (LLMs) sont des mod√®les de traitement du langage naturel (NLP) caract√©ris√©s par un tr√®s grand nombre de param√®tres (allant de plusieurs milliards √† des centaines de milliards).
            Cette √©chelle leur permet de g√©rer des t√¢ches complexes en capturant des relations fines dans de grandes quantit√©s de donn√©es textuelles. Ils sont bas√©s sur l'architecture des Transformers, 
            qui exploitent le m√©canisme d'attention pour mod√©liser efficacement les relations contextuelles entre les mots, m√™me sur de longues s√©quences textuelles.
            </div>
        """, unsafe_allow_html=True)

    elif section_deep_learning == "Cas d'usage":
        st.header("Chatbot pour la Suite Office")
        st.write("""
            <div style="text-align: justify;">
            Cet assistant est con√ßu pour aider les utilisateurs des logiciels de la suite Microsoft Office. 
            En exploitant les capacit√©s des mod√®les de langage, elle permet de r√©pondre en temps r√©el √† des questions techniques sp√©cifiques concernant les outils tels que Word, Excel ou PowerPoint.
            Ce chatbot offre une aide personnalis√©e et un guidage pas-√†-pas pour optimiser l'efficacit√© des utilisateurs dans leurs t√¢ches quotidiennes.
            </div>
        """, unsafe_allow_html=True)

        import os
        import streamlit as st
        from streamlit_chat import message
        from langchain.chains import ConversationChain
        from langchain_community.llms import OpenAI  # Utilisation de l'import correct


        # Cl√© API OpenAI (Remplacer par la cl√© API valide)
        openai_api_key = os.getenv("MY_API_KEY")

        if not openai_api_key:
            st.error("Cl√© API OpenAI introuvable. Veuillez la d√©finir.")
        else:
        # Charger la cha√Æne OpenAI
            #lm = OpenAI(temperature=0, openai_api_key=openai_api_key)
            llm = OpenAI(temperature=0, openai_api_key=openai_api_key)
            chain = ConversationChain(llm=llm)

        # Initialisation de l'√©tat de session pour stocker les messages
            if "messages" not in st.session_state:
                st.session_state["messages"] = []

        # Cr√©er un formulaire pour la saisie utilisateur
            with st.form(key="user_input_form"):
                user_input = st.text_input("Vous : ", "")
                submit_button = st.form_submit_button(label="Envoyer")

            # Si l'utilisateur envoie une nouvelle question
            if submit_button and user_input:
            # Ajouter le message utilisateur √† la session
                st.session_state["messages"].append({"role": "user", "content": user_input})

                try:
                # G√©n√©rer une r√©ponse avec OpenAI
                    prompt = f"{user_input}\nR√©pondez toujours en fran√ßais."
                    output = chain.run(input=prompt)

                # Ajouter la r√©ponse du chatbot √† la session
                    st.session_state["messages"].append({"role": "bot", "content": output})
                except Exception as e:
                    st.error(f"Erreur lors de la g√©n√©ration de la r√©ponse : {str(e)}")

            # Afficher les messages dans le chat avec des cl√©s uniques pour chaque message
            for i, msg in enumerate(st.session_state["messages"]):
                if msg["role"] == "user":
                    message(msg["content"], is_user=True, key=f"user_{i}")
                else:
                    message(msg["content"], key=f"bot_{i}")
